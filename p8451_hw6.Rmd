---
title: "Machine Learning for Epi: Assignment 6"
output:
  html_document: default
  word_document: default
date: "2023-02-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = F,
                      message = F)

library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(NHANES)
library(e1071)
```

## Description of Data

The data we will be using are from the from the 2009-2012 National Health and Nutrition Examination Survey (NHANES), accessed through the `NHANES` package in R. *We will be using this dataset to try to predict diabetes and pre-diabetes.* We have restricted the dataset to 11 features and an outcome which distinguishes those who report diabetes status (defined as study participant told by a doctor or health professional that they have diabetes).

### Step 1: Load data and prepare for analysis

The code chunk below loads the NHANES data, omits missing observations, and converts the variables Race1, Education, HHIncome, Diabetes, PhysActive, and Smoke100 to factor variables. 

```{r load_data}
data("NHANES")

NHANES_df = NHANES %>% 
  dplyr::select(Age, Race1, Education, HHIncome, Weight, Height, Pulse, Diabetes, BMI, PhysActive, Smoke100) %>% 
  mutate(Race1 = as.factor(Race1), 
         Education = as.factor(Education),
         HHIncome = as.factor(HHIncome), 
         Diabetes = as.factor(Diabetes), 
         PhysActive = as.factor(PhysActive), 
         Smoke100 = as.factor(Smoke100)) %>% 
  drop_na()

summary(NHANES_df)
```

Our resulting variables include 11 demographic and health features, and our binary outcome variable, `Diabetes`. Based on the summary, we can see that the distribution of diabetes is quite unbalanced, with a 10.37% prevalence of the outcome. 

### Step 2: Partition the data 

The code chunk below partitions the data into training and testing sets, using a 70/30 split. 

```{r partition_data}
set.seed(123)

#Creating balanced partitions in the data
train_index = createDataPartition(NHANES_df$Diabetes, p = 0.7, list = FALSE)

NHANES_train = NHANES_df[train_index,]
NHANES_test = NHANES_df[-train_index,]

#Check distribution of the outcome between train and test data
summary(NHANES_train$Diabetes) 
summary(NHANES_test$Diabetes)
```

We can see that there are similar distributions of the variable `diabetes`, with approximately 10% prevalence of cases of diabetes across both the training and testing sets, indicating that the data were successfully partitioned.

### Step 3: Construct logistic regression models to predict healthy days

We will fit 3 prediction models to predict diabetes status. (feature name: `Diabetes`).

- Model 1 (`class_tree`): Classification Tree

- Model 2 (`mod_svc`): Support Vector Classifier (i.e. Support Vector Machine with a linear classifier)

- Model 3 (`mod_log`): A logistic model based on all features.

#### Classification Tree

To fit the classification tree, we will train using 10-fold cross-validation, and set the tune grid to values ranging from 0.001 to 0.3, searching in increments of 0.01.

```{r classtree}
set.seed(123)

#Creating 10-fold cross-validation and using down-sampling because of imbalance in data
train.control.class = trainControl(method = "cv", number = 10, sampling = "down")

#Create sequence of cp parameters to try 
grid.2 = expand.grid(cp = seq(0.001, 0.3, by = 0.01))

#Train model
class_tree = train(Diabetes ~ ., data = NHANES_train, method = "rpart", trControl = train.control.class, tuneGrid = grid.2)
class_tree$bestTune

class_tree

rpart.plot(class_tree$finalModel)

#Note you can obtain variable importance on the final model within training data
varImp(class_tree)

#Note you can get accuracy metric and confusion matrix from training.
confusionMatrix(class_tree)

# Get results
class_perf = class_tree$results %>% arrange(desc(Accuracy)) %>% slice(1) %>% 
    rename(C = "cp")
```

The resulting model found a relatively small complexity parameter Cp of 0.001. We can see that the most important variables in the classification tree, respectively are Age, BMI, and Weight, and as such these are the features that are split near the top of the tree. The confusion matrix indicates that across 10 cross-validated folds of the data, the model has a classification accuracy of 70.45%.

#### SVC Model

To fit the SVC model, we will train using 10-fold cross-validation, and set the tune grid to values ranging from 0.001 to 2 of a length 30 values.

```{r mod_svc}
set.seed(123)

#Set 10-fold cross-validation and downsample
train_control = trainControl(method = "cv", number = 10, sampling = "down", classProbs = T)

#Incorporate different values for cost (C)
mod_svc = train(Diabetes ~ ., data = NHANES_train, method = "svmLinear",  trControl = train_control, preProcess = c("center", "scale"), tuneGrid = expand.grid(C = seq(0.001, 2, length = 30)))

mod_svc$bestTune

#Visualize accuracy versus values of C
plot(mod_svc)

#Obtain metrics of accuracy from training
confusionMatrix(mod_svc)

#See information about final model
mod_svc$finalModel
# Get results
svc_perf = mod_svc$results %>% arrange(desc(Accuracy)) %>% slice(1) 
```

The resulting model found a relatively smallparameter C of 0.001. We can see that the most important variables in the classification tree, respectively are Age, BMI, and Weight, and as such these are the features that are split near the top of the tree. The confusion matrix indicates that across 10 cross-validated folds of the data, the model has a classification accuracy of 70.45%.


#### Logistic Regression Model

To fit the logisic model, we will train the model within caret on the training dataset.

```{r mod_logistic}
set.seed(123)

mod_log = train(Diabetes ~ ., data = NHANES_train, method = "glm")
mod_log

log_perf = mod_log$results %>% arrange(desc(Accuracy)) %>% slice(1) %>% 
  rename(C = "parameter") %>% 
  mutate(C = as.numeric(C))
```

The resulting model includes all features to be fed into the model, with Impulsivity and Sensation-Seeking Behaviors having the largest effect size as indicated by the magnitude of their coefficients.

#### Comparing performance across models

Finally, let's compare the performance of the 3 models.

```{r compare}
rbind(class_perf, log_perf, svc_perf) %>% 
  mutate(Model = c("Classification Tree", "Logistic Regression", "SVC")) %>% 
  relocate(Model) %>% 
  rename(Hyperparameter = C) %>% 
  arrange(desc(Accuracy)) %>% 
  knitr::kable(digits = 4)
```

The table shows that the baseline Logistic Regression Model has the best model performance as measured by accuracy (89.28%%), followed narrowly by the SVC model (72.45%), then the Classification Tree (70.22%). If I was interested in making sure I maximize accuracy, I would go with the Logistic Regression model, and would select this model to classify diabetes status.

### Step 4: Final Model Evaluation

Finally, we will evaluate the performance our final Elastic Net model by making predictions in the test data. We will use the `confusionMatrix()` function to get performance measures of accuracy, kappa, sensitivity, specificity, and precision (PPV) for the model.

```{r test_EN}
# Make predictions in test set
pred = mod_log %>% predict(NHANES_test)
NHANES_test = NHANES_test %>% mutate(pred = as.factor(pred))

# Model prediction performance
cm = confusionMatrix(data = NHANES_test$pred, reference = NHANES_test$Diabetes, positive = "Yes")

cbind(cm$overall %>% as_tibble_row(), cm$byClass %>% as_tibble_row()) %>% 
  dplyr::select(Accuracy, Kappa, Sensitivity, Specificity, Precision) %>% 
  knitr::kable()
```

On the testing set, we can see that the accuracy of the Logistic Regression model is 89.56%. Moreover, we can see the sensitivity of the model is ...%, with a specificity of ...%. This indicates that the model is ..., but ....

### Research Applications

This analysis could directly address the research question of which behavioral features best predict current alcohol consumption, and therefore narrow down the list of behavioral tests clinicians may need to administer to patients in order to carry out this predictive task. One such application could be applying this predictive model to patients' electronic health records to predict current alcohol consumption among patients on the basis of behavioral test scores. Of course, this comes with several ethical concerns, such as considering the harms enacted on patients, notably if the model we selected above has a  tendency to report false positives in detecting current alcohol consumption.
